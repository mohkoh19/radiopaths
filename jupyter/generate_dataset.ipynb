{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pneumonia Multimodal Dataset Generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our main goal in this notebook is to utilize as much data as possible from the vast amount available at MIMIC-IV in a multimodal manner. We want to investigate how incorporating clinical data affects the overall classification of diseases."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "Please make sure that the following requirements are met before executing the cells below:\n",
    "- Credentialed access to [Physionet](https://physionet.org/) (after registering you have to apply for the [credentialing process](https://physionet.org/settings/credentialing/) and get accepted)\n",
    "- Request access for MIMIC-IV, MIMIC-CXR, and MIMIC-CXR-JPG\n",
    "- Request access for MIMIM-IV and MIMIC-CXR using Google BigQuery (a Google Account is needed for this. You can find more information on the [MIMIC website](https://mimic.mit.edu/docs/gettingstarted/cloud/))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "In this section the needed packages are imported and some helper functions are defined. We also specify important variables as described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_gbq as gbq\n",
    "import os\n",
    "import pydata_google_auth\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer\n",
    "from multiprocessing import cpu_count, Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Cloud authentication\n",
    "# Needed to query data from MIMIC-IV\n",
    "\n",
    "SCOPES = [\n",
    "    'https://www.googleapis.com/auth/cloud-platform',\n",
    "    'https://www.googleapis.com/auth/drive',\n",
    "]\n",
    "\n",
    "# Set auth_local_webserver to True to have a slightly more convienient \n",
    "# authorization flow. Note, this doesn't work if you're running from a \n",
    "# notebook on a remote sever, such as over SSH or with Google Colab. \n",
    "credentials = pydata_google_auth.get_user_credentials(\n",
    "    SCOPES,\n",
    "    use_local_webserver=False,\n",
    ")\n",
    "\n",
    "gbq.context.credentials = credentials\n",
    "\n",
    "# The id of the GCP project that is connected to physionet in GBQ\n",
    "# project_id = 'my-project-1234567'\n",
    "project_id = 'master-thesis-332120'\n",
    "\n",
    "# Physionet authentication\n",
    "# Needed to download reports and CXRs from MIMIC-CXR\n",
    "# user = 'my-user'\n",
    "# password = 'my-passwd'\n",
    "user = 'mohkoh'\n",
    "password = 'Gawhak-zawpoz-1tunja'\n",
    "\n",
    "# Don't change this\n",
    "base_url = \"https://physionet.org/files/\"\n",
    "\n",
    "# The image resolution R for the CXR images. \n",
    "# The original images will be downscaled to R x R\n",
    "image_resolution = 225\n",
    "\n",
    "# Set path to the local working directory\n",
    "# If the path does not exist it will be created\n",
    "# The generated dataframes, study reports, etc. will be copied here\n",
    "# Moving files out of the directory or renaming them during the \n",
    "# execution of this notebook might lead to errors\n",
    "# local_dir = '.local_dir'\n",
    "local_dir = '/home/mohammad/Projects/master-thesis/frames'\n",
    "os.makedirs(local_dir, exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some helper functions to run queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(query, project_id=project_id):\n",
    "  \"\"\"Runs SQL query in GBQ and returns result as pandas DataFrame\n",
    "\n",
    "  Args:\n",
    "      query (string): Query in SQL Standard dialect.\n",
    "      project_id (string, optional): The project-id from GCP to use. Defaults to a global variable named project_id.\n",
    "\n",
    "  Returns:\n",
    "      [type]: [description]\n",
    "  \"\"\"\n",
    "  return gbq.read_gbq(query, project_id=project_id, dialect='standard')\n",
    "\n",
    "def lazy_run_query(local_path, query=None, save_local=True, project_id=project_id, transform=None):\n",
    "  \"\"\"Runs a query if no local version (as .csv) exists. Only tries to load local version if no query is specified. \n",
    "\n",
    "  Args:\n",
    "      local_path (string): Path of the local version to look for.\n",
    "      query (string): Query in SQL Standard dialect.\n",
    "      save_local (bool, optional): Whether or not the result should be saved locally. Defaults to True.\n",
    "      project_id (string, optional): The project-id from GCP to use. Defaults to a global variable named project_id.\n",
    "      transform (func, optional): If specified, the function is called on the queried/loaded result. \n",
    "                                  The function should expect a pandas DataFrame as only parameter.\n",
    "                                  The function should only return a pandas DataFrame. Defaults to None.\n",
    "\n",
    "  Returns:\n",
    "      [type]: [description]\n",
    "  \"\"\"\n",
    "  local_exists = os.path.isfile(local_path)\n",
    "  if (local_exists):\n",
    "    result = pd.read_csv(local_path)\n",
    "  elif (query is not None):\n",
    "    result = run_query(query, project_id=project_id)\n",
    "  else:\n",
    "    return None\n",
    "    \n",
    "  if (save_local and not local_exists):\n",
    "    result.to_csv(local_path, index=False)\n",
    "    \n",
    "  if(transform is not None):\n",
    "    result = transform(result)\n",
    "\n",
    "  return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "In this section we retrieve all the information that we need from MIMIC-CXR/IV and filter according to our cohort definition. We rely on `pandas` for all data manipulation steps as well as the execution of SQL queries on the BigQuery instance. Thanks to the predefined concepts in `mimic-derived`, in most cases we can simply query whole tables.\n",
    "\n",
    "\n",
    "Our goal for this section is to bring forth a dataset consisting of:\n",
    "- Patient demographics (age, gender)\n",
    "- Complete Bloodcount (CBC) specimen (max. 3 days old)\n",
    "- Vital signs (VIT) such as oxigen satisfaction or temperature (max. 24 hours)\n",
    "- Radiology Report Indication Section\n",
    "  \n",
    "We argue that the chosen sources provide a good tradeoff between utility and availability with regards to our classification task."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIMIC-CXR and ICD-Codes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this subsection we merge all CXR related tables and explain the most relevant tables and attributes. The table `study_list` contains a list of all patients and studies with their respective reports (as relative paths). The table `record_list` is analogue to `study_list`, while containing related X-Ray images instead of reports (again as relative pahts). Note that the same patient can have multiple different images for the same study, due to multiple view positions. Lastly, the `dicom_meta_string` table contains  meta information for each image. In particular, we are interested in the timing of the studies to select appropriate covariates in later steps, and the view position of the image. \n",
    "\n",
    "Each row then contains at least the following attributes:\n",
    "- `subject_id` (identifier for patient)\n",
    "- `study_id` (identifier for radiology study)\n",
    "- `study_datetime` (date and time when image was taken)\n",
    "- `report_path` (relative path of the report)\n",
    "- `image_path` (relative path of the image)\n",
    "- `view_position` (position from which the image was taken)\n",
    "\n",
    "There is another MIMIX-CXR related table, namely `chexpert`, which contains 14 labels per image generated by the [chexpert-labeler](https://github.com/stanfordmlgroup/chexpert-labeler), a NLP tool that is based on Negation and tries to retrieve lung disease and findings labels from study reports (for more information check the link). Our task is to classify `Pneumonia`, but we want to make our classifier robust agains ambiguities between similar visual features of different diseases. Therefore, we include all of the 14 labels available:\n",
    "- Atelectasis\n",
    "- Cardiomegaly\n",
    "- Consolidation\n",
    "- Edema\n",
    "- Enlarged_Cardiomediastinum\n",
    "- Fracture\n",
    "- Lung_Lesion\n",
    "- Lung_Opacity\n",
    "- No_Finding\n",
    "- Pleural_Effusion\n",
    "- Pleural_Other\n",
    "- Pneumonia\n",
    "- Pneumothorax\n",
    "- Support_Devices\n",
    "\n",
    "Each label contains one of four values: 1.0, âˆ’1.0, 0.0, or NaN, which indicate positive, negative, uncertain, or missing observations, respectively. \n",
    "\n",
    "Lastly, we add the `age` and `gender` information of the respective patients to each entity by joining the `patients` table from MIMIC-Core on the `subject_id`\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An issue with the chexpert-labeler (and other NLP tools for label extraction) is the huge amount of uncertain and missing labels, which reduces the amount of data we can use significantly. Just substituting those labels with negative or positive ones could lead to a substantial increase in false negative rates and corrupt or bias subsequent results.\n",
    "\n",
    "As described in the paper, we try to fill uncertain or missing labels in our preprocessing by using the available ICD-9 and ICD-10 ontologies from `diagnoses_icd` and `d_icd_diagnoses` tables from `mimic_hosp`. This approach is only possible for diseases that can be described with ICD-codes, not for mere findings in the radiograph images. ICD codes can only be identified by the `subject_id` and `hadm_id` of the respective patients. We have no information about the time of a diagnosis, but only a list of all diagnoses per stay in the hospital. Thus we can't use the ICD codes to determine positive labels and are also limited in determining negative labels i.e. we can only globally derive labels for the whole hospitalization and not for each study during the same hospitalization.\n",
    "\n",
    "Unfortunatly, we don't have the `hadm_id` for the CXR studies. In fact, we can not even assume each study to have a `hadm_id`, since not every study was performed during an admission to the hospital. However, through the `hadm_id` we can retrieve the admission time and discharging time (`admittime` and `dischtime`) for each patient and each admission respectively. We can use these information to check for each study if it was conducted inbetween, since we have the `study_datetime` for each study. \n",
    "\n",
    "For each match we concatenate all the corresponding `long_title` values from `d_icd_diagnoses`, which serve as descriptions for the associated ontologies.  Lastly, we search in the descriptions for keywords indicating `Pneumonia`.\n",
    "\n",
    "If the disease is not included in the extracted list of diagnoses, we set the label to 0. In particular, we don't overwrite existing certain positive labels from the `chexpert-labeler`, since it has a higher specificity than ICD based diagnoses.\n",
    "\n",
    "If the disease is included, we change the label to 1 only if other co-occurences are given (see `handle_missing_labels` in `src/datasets/preprocessing.py`). This is due to the fact, that we don't know at which time the patients have been diagnosed. At best the ICD ontology increases the likelihood of the disease. This can still lead to many false positives in our labels.  \n",
    "\n",
    "The remaining missing labels are mapped to `0`. For the remaining uncertain labels we try both binary mappings, to `0` and to `1`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 376207/376207 [03:45<00:00, 1669.00rows/s]\n"
     ]
    }
   ],
   "source": [
    "mimic_cxr = lazy_run_query(os.path.join(local_dir, \"mimic_cxr.csv\"), query=\n",
    "\"\"\" \n",
    "    WITH patients AS\n",
    "    (\n",
    "        SELECT *\n",
    "        FROM `physionet-data.mimic_core.patients`\n",
    "    )\n",
    "    , study_list AS\n",
    "    (\n",
    "        SELECT subject_id, study_id, path AS report_path\n",
    "        FROM `physionet-data.mimic_cxr.study_list`\n",
    "    )\n",
    "    , dicom_meta AS\n",
    "    (\n",
    "        SELECT dicom AS dicom_id\n",
    "        , PARSE_DATETIME('%Y%m%d %H%M%E*S', CONCAT(StudyDate, ' ', StudyTime)) AS study_datetime\n",
    "        , ViewPosition AS view_position\n",
    "        FROM `physionet-data.mimic_cxr.dicom_metadata_string`\n",
    "    )\n",
    "    , chexpert AS\n",
    "    (\n",
    "        SELECT subject_id, study_id\n",
    "        , Atelectasis\n",
    "        , Cardiomegaly\n",
    "        , Consolidation\n",
    "        , Edema\n",
    "        , Enlarged_Cardiomediastinum\n",
    "        , Fracture\n",
    "        , Lung_Lesion\n",
    "        , Lung_Opacity\n",
    "        , No_Finding\n",
    "        , Pleural_Effusion\n",
    "        , Pleural_Other\n",
    "        , Pneumonia\n",
    "        , Pneumothorax\n",
    "        , Support_Devices\n",
    "        FROM `physionet-data.mimic_cxr.chexpert`\n",
    "    ),\n",
    "    diag_adm AS (\n",
    "        WITH diagnoses AS (\n",
    "            SELECT hadm_id\n",
    "            , STRING_AGG(long_title ORDER BY long_title) AS diagnoses_text\n",
    "            FROM `physionet-data.mimic_hosp.diagnoses_icd` AS diagnoses_icd\n",
    "            JOIN `physionet-data.mimic_hosp.d_icd_diagnoses` AS d_icd_diagnoses \n",
    "            ON diagnoses_icd.icd_code = d_icd_diagnoses.icd_code\n",
    "            GROUP BY hadm_id \n",
    "        ) \n",
    "        SELECT subject_id\n",
    "        , admittime\n",
    "        , dischtime\n",
    "        , diagnoses_text\n",
    "        FROM `physionet-data.mimic_core.admissions` AS admissions\n",
    "        JOIN diagnoses ON diagnoses.hadm_id = admissions.hadm_id\n",
    "    )\n",
    "    SELECT record_list.subject_id\n",
    "    , record_list.study_id\n",
    "    , anchor_age\n",
    "    , gender\n",
    "    , study_datetime\n",
    "    , report_path\n",
    "    , record_list.dicom_id\n",
    "    , path AS image_path\n",
    "    , view_position\n",
    "    , Atelectasis\n",
    "    , Cardiomegaly\n",
    "    , Consolidation\n",
    "    , Edema\n",
    "    , Enlarged_Cardiomediastinum\n",
    "    , Fracture\n",
    "    , Lung_Lesion\n",
    "    , Lung_Opacity\n",
    "    , No_Finding\n",
    "    , Pleural_Effusion\n",
    "    , Pleural_Other\n",
    "    , Pneumonia\n",
    "    , Pneumothorax\n",
    "    , Support_Devices\n",
    "    , LOWER(diagnoses_text) AS diagnoses_text\n",
    "    FROM `physionet-data.mimic_cxr.record_list` record_list\n",
    "    INNER JOIN patients ON record_list.subject_id = patients.subject_id\n",
    "    INNER JOIN dicom_meta ON record_list.dicom_id = dicom_meta.dicom_id\n",
    "    INNER JOIN study_list ON record_list.subject_id = study_list.subject_id AND record_list.study_id = study_list.study_id\n",
    "    INNER JOIN chexpert ON record_list.subject_id = chexpert.subject_id AND record_list.study_id = chexpert.study_id\n",
    "    LEFT OUTER JOIN diag_adm \n",
    "        ON record_list.subject_id = diag_adm.subject_id \n",
    "        AND dicom_meta.study_datetime BETWEEN diag_adm.admittime AND diag_adm.dischtime\n",
    "    WHERE record_list.dicom_id IS NOT NULL\n",
    "    ORDER BY study_id\n",
    "    ;\n",
    "\"\"\", save_local=True, transform=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clinical Covariates \n",
    "In this subsection we want wo add relevant clinical covariates as additional modality to support our deep learning model. In particular, we are interested in the latest complete blood count test (CBC) and the latest vitalsigns of the patient prior to the corresponding study date. Of course, we can't expect every patient to have the information available before the image was taken. However, considering the clinical workflow it is very likely that such data is acquired before the imaging (at least for patients with symptpoms for our chosen group of diseases). As our task is to investigate the impact of the clinical information on disease classification by image, we only consider those entities for our cohort, which have a CBC not older than 3 days and vitalsigns not older than 24 hours before the study date available. In particular, we add the following features to our dataset:\n",
    "\n",
    "Complete Blood Count (CBC):\n",
    "- `hematocrit` (volume percentage of red blood cells in blood)\n",
    "- `hemoglobin` (protein that carries oxygen through the blood)\n",
    "- `mch` (mean corpuscular hemoglobin, average mass of hemoglobin per red blood cell)\n",
    "- `mchc` (mean corpuscular hemoglobin concentration, calculated by dividing the hemoglobin by the hematocrit)\n",
    "- `mcv` (mean corpuscular volume, average volume of a red blood corpuscle)\n",
    "- `platelet` (cell fragments that form clots and stop or prevent bleeding)\n",
    "- `rbc` (red blood cells)\n",
    "- `rdw` (red blood cell distribution width)\n",
    "- `wbc` (white blood cells)\n",
    "\n",
    "Vitalsigns:\n",
    "- `heart_rate` \n",
    "- `dbp` (diastolic blood pressure)\n",
    "- `sbp` (systolic blood pressure)\n",
    "- `mbp` (median blood pressure)\n",
    "- `resp_rate` (respiration rate)\n",
    "- `temperature` (body temperature)\n",
    "- `spo2` (peripheral capillary oxygen saturation)\n",
    "\n",
    "Fortunatly, we find both abstractions inside the `mimic_derived` module. So we simply subselect our chosen features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55890/55890 [00:11<00:00, 4772.95rows/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 183496/183496 [00:40<00:00, 4533.01rows/s]\n"
     ]
    }
   ],
   "source": [
    "vit_path = os.path.join(local_dir, \"vit_raw.csv\")\n",
    "cbc_path = os.path.join(local_dir, \"cbc_raw.csv\")\n",
    "\n",
    "vit_raw = lazy_run_query(\n",
    "    vit_path,\n",
    "    \"\"\" WITH max_times AS (\n",
    "        WITH cxr AS\n",
    "        (\n",
    "            SELECT subject_id, study_id, dicom_id, study_datetime\n",
    "            FROM `physionet-data.mimic_cxr.record_list`\n",
    "            INNER JOIN (\n",
    "                SELECT dicom, PARSE_DATETIME('%Y%m%d %H%M%E*S', CONCAT(StudyDate, ' ', StudyTime)) AS study_datetime\n",
    "                FROM `physionet-data.mimic_cxr.dicom_metadata_string`\n",
    "            )\n",
    "            ON dicom_id = dicom\n",
    "        )\n",
    "        SELECT vit.subject_id\n",
    "        , cxr.study_datetime\n",
    "        , cxr.study_id\n",
    "        , MAX(vit.charttime) AS vit_charttime\n",
    "        FROM `physionet-data.mimic_derived.vitalsign` AS vit,\n",
    "        cxr\n",
    "        WHERE cxr.subject_id = vit.subject_id \n",
    "        AND study_datetime >= vit.charttime\n",
    "        AND DATE_DIFF(CAST(study_datetime AS DATE), CAST(vit.charttime AS DATE), DAY) <= 1\n",
    "        AND resp_rate IS NOT NULL\n",
    "        AND heart_rate IS NOT NULL\n",
    "        AND temperature IS NOT NULL\n",
    "        AND spo2 IS NOT NULL\n",
    "        GROUP BY vit.subject_id\n",
    "        , cxr.study_datetime\n",
    "        , cxr.study_id\n",
    "    )\n",
    "    SELECT vit.subject_id\n",
    "    , max_times.study_datetime\n",
    "    , max_times.study_id\n",
    "    , vit.charttime AS vit_charttime\n",
    "    , heart_rate\n",
    "    , sbp\n",
    "    , dbp\n",
    "    , mbp\n",
    "    , resp_rate\n",
    "    , temperature\n",
    "    , spo2\n",
    "    FROM `physionet-data.mimic_derived.vitalsign` AS vit\n",
    "    JOIN max_times\n",
    "    ON max_times.subject_id = vit.subject_id\n",
    "    AND max_times.vit_charttime = vit.charttime\n",
    "\"\"\",\n",
    "    save_local=True,\n",
    ")\n",
    "\n",
    "cbc_raw = lazy_run_query(\n",
    "    cbc_path,\n",
    "    \"\"\" WITH max_times AS (\n",
    "        WITH cxr AS\n",
    "        (\n",
    "            SELECT subject_id, study_id, dicom_id, study_datetime\n",
    "            FROM `physionet-data.mimic_cxr.record_list`\n",
    "            INNER JOIN (\n",
    "                SELECT dicom, PARSE_DATETIME('%Y%m%d %H%M%E*S', CONCAT(StudyDate, ' ', StudyTime)) AS study_datetime\n",
    "                FROM `physionet-data.mimic_cxr.dicom_metadata_string`\n",
    "            )\n",
    "            ON dicom_id = dicom\n",
    "        )\n",
    "        SELECT cbc.subject_id\n",
    "        , cxr.study_datetime\n",
    "        , cxr.study_id\n",
    "        , MAX(cbc.charttime) AS cbc_charttime\n",
    "        FROM `physionet-data.mimic_derived.complete_blood_count` AS cbc, \n",
    "        cxr\n",
    "        WHERE cxr.subject_id = cbc.subject_id\n",
    "        AND study_datetime >= cbc.charttime\n",
    "        AND DATE_DIFF(CAST(study_datetime AS DATE), CAST(cbc.charttime AS DATE), DAY) <= 3 \n",
    "        AND hematocrit IS NOT NULL\n",
    "        AND hemoglobin IS NOT NULL\n",
    "        AND mch IS NOT NULL\n",
    "        AND mchc IS NOT NULL\n",
    "        AND mcv IS NOT NULL\n",
    "        AND platelet IS NOT NULL\n",
    "        AND rbc IS NOT NULL\n",
    "        AND rdw IS NOT NULL\n",
    "        AND wbc IS NOT NULL\n",
    "        GROUP BY cbc.subject_id\n",
    "        , cxr.study_datetime\n",
    "        , cxr.study_id\n",
    "    )\n",
    "    SELECT cbc.subject_id\n",
    "    , max_times.study_datetime\n",
    "    , max_times.study_id\n",
    "    , cbc.charttime AS cbc_charttime\n",
    "    , hematocrit\n",
    "    , hemoglobin\n",
    "    , mch\n",
    "    , mchc\n",
    "    , mcv\n",
    "    , platelet\n",
    "    , rbc\n",
    "    , rdw\n",
    "    , wbc\n",
    "    FROM `physionet-data.mimic_derived.complete_blood_count` AS cbc\n",
    "    JOIN max_times\n",
    "    ON max_times.subject_id = cbc.subject_id\n",
    "    AND max_times.cbc_charttime = cbc.charttime\n",
    "\"\"\",\n",
    "    save_local=True,\n",
    "    transform=lambda df: df.dropna(),\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clinical Notes (Radiology Report Indication Sections)\n",
    "In this subsection we add free text that was produced by the clinicians in the context of the study. This may include physical exams, clinical notes, study reports etc. \n",
    "\n",
    "Currently, we only have the radiology report available. This report is written by the radiologist *after* examining the X-Ray image of the patients. From the perspective of our classification task it doesn't make sense to include information generated by the radiologist, since this information is not available at the time we see the X-Ray image. \n",
    "\n",
    "The only exception is the indication section of a radiology report, which is written by the doctor, who ordered the X-Ray for the patient. To extract the indication sections, we first download all reports from Physionet. We then use a script from the [MIMIC Code Repository](https://github.com/MIT-LCP/mimic-code) to extract all indication sections. Please note that we use a [fork of the repository](https://github.com/mohkoh19/mimic-cxr), in which the `create_section_files.py` script has been modified to include the indication sections next to the impressions and findings sections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_file = os.path.join(base_url, \"mimic-cxr/2.0.0/mimic-cxr-reports.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-12-20 16:06:31--  https://physionet.org/files/mimic-cxr/2.0.0/mimic-cxr-reports.zip\n",
      "Resolving physionet.org (physionet.org)... 18.18.42.54\n",
      "Connecting to physionet.org (physionet.org)|18.18.42.54|:443... connected.\n",
      "HTTP request sent, awaiting response... 401 Unauthorized\n",
      "Authentication selected: Basic realm=\"PhysioNet\", charset=\"UTF-8\"\n",
      "Reusing existing connection to physionet.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 141942511 (135M) [application/zip]\n",
      "Saving to: â€˜/home/mohammad/Projects/master-thesis/sandbox/mimic-cxr-reports.zipâ€™\n",
      "\n",
      "mimic-cxr-reports.z 100%[===================>] 135,37M  4,36MB/s    in 30s     \n",
      "\n",
      "2022-12-20 16:07:02 (4,49 MB/s) - â€˜/home/mohammad/Projects/master-thesis/sandbox/mimic-cxr-reports.zipâ€™ saved [141942511/141942511]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -q --user {user} --password {password} {reports_file} -P {local_dir}\n",
    "!unzip -q {os.path.join(local_dir, \"mimic-cxr-reports.zip\")} -d {local_dir} && rm {os.path.join(local_dir, \"mimic-cxr-reports.zip\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'mimic-cxr' already exists and is not an empty directory.\n",
      "p10\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6397/6397 [00:01<00:00, 3742.07it/s]\n",
      "p11\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6571/6571 [00:01<00:00, 3935.57it/s]\n",
      "p12\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6528/6528 [00:01<00:00, 4200.50it/s]\n",
      "p13\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6550/6550 [00:01<00:00, 4049.32it/s]\n",
      "p14\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6507/6507 [00:01<00:00, 4165.24it/s]\n",
      "p15\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6593/6593 [00:01<00:00, 3880.04it/s]\n",
      "p16\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6476/6476 [00:01<00:00, 4260.46it/s]\n",
      "p17\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6644/6644 [00:01<00:00, 3945.72it/s]\n",
      "p18\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6543/6543 [00:01<00:00, 4143.89it/s]\n",
      "p19\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6579/6579 [00:01<00:00, 3908.03it/s]\n"
     ]
    }
   ],
   "source": [
    "!cd {local_dir} && git clone git@github.com:mohkoh19/mimic-cxr.git\n",
    "!cd {local_dir} && python mimic-cxr/txt/create_section_files.py --reports_path ./files --output_path . --no_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "clino_raw = pd.read_csv(\n",
    "    os.path.join(local_dir, \"mimic_cxr_sectioned.csv\")\n",
    ")[[\"study\", \"indication\"]]\n",
    "clino_raw.rename(columns={\"study\": \"study_id\", \"indication\": \"notes\"}, inplace=True)\n",
    "clino_raw[\"study_id\"] = clino_raw[\"study_id\"].str[1:].astype(int)\n",
    "clino_raw = clino_raw.dropna().reset_index(drop=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After extracting the notes and applying some basic preprocessing, we use a [tokenizer provided by the authors of BioBERT](https://huggingface.co/dmis-lab/biobert-v1.1) on Hugging Face to tokenize the notes and save them as tokens instead of strings. This will save us a lot of runtime during training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "clino = clino_raw.copy()\n",
    "\n",
    "# Transform notes to single lower-case sentence, remove anonymised info\n",
    "notes = clino[\"notes\"]\n",
    "notes = notes.str.lower()\n",
    "notes.replace(r\"\\n\", \" \", regex=True, inplace=True)\n",
    "notes.replace(r\"[^\\w\\s]\", \"\", regex=True, inplace=True)\n",
    "notes.replace(r\"___\", \" \", regex=True, inplace=True)\n",
    "notes.replace(r\" *(year old|y o|yo|yearold)\", \"\", regex=True, inplace=True)\n",
    "clino[\"notes\"] = notes\n",
    "\n",
    "# Tokenize all notes and attach to dataframe\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "tokens = tokenizer(notes.to_list(), padding=\"max_length\", truncation=True)\n",
    "df_tokens = pd.DataFrame.from_dict(tokens, orient=\"index\").T\n",
    "clino = clino.join(df_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "clino.to_pickle(os.path.join(local_dir, 'clino.pkl'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Datasets\n",
    "\n",
    "We merge all datasets to a final multivariate dataset that can be used for all of our models, namely `mimic_cxr_mv.pkl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_cxr = pd.read_csv(os.path.join(local_dir, \"mimic_cxr.csv\"))\n",
    "clino = pd.read_pickle(os.path.join(local_dir, \"clino.pkl\"))\n",
    "cbc_raw = pd.read_csv(os.path.join(local_dir, \"cbc_raw.csv\"))\n",
    "vit_raw = pd.read_csv(os.path.join(local_dir, \"vit_raw.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incase we read from csv datetime type is not preserved, run this cell to match all datetime types\n",
    "mimic_cxr[\"study_datetime\"] = pd.to_datetime(mimic_cxr[\"study_datetime\"])\n",
    "cbc_raw[\"study_datetime\"] = pd.to_datetime(cbc_raw[\"study_datetime\"])\n",
    "vit_raw[\"study_datetime\"] = pd.to_datetime(vit_raw[\"study_datetime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_cxr_mv = pd.merge(mimic_cxr, cbc_raw, on=['study_id', 'subject_id', 'study_datetime'], how=\"left\")\n",
    "mimic_cxr_mv = pd.merge(mimic_cxr_mv, vit_raw, on=['study_id', 'subject_id', 'study_datetime'], how=\"left\")\n",
    "mimic_cxr_mv = pd.merge(mimic_cxr_mv, clino, on=['study_id'], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_cxr_mv.to_pickle(os.path.join(local_dir, \"mimic_cxr_mv.pkl\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Downscale CXRs (JPEG)\n",
    "The `mimic_cxr_mv.pkl` dataset does not include the CXR images, but only relative file paths. Thus, it is expected that the local file structure resembles the remote file structure on physionet. The cell below downloads all CXR images in the JPEG format from MIMIC-CXR-JPG in the given resolution and saves them at the correct location in the `files/` folder that has been extracted earlier for the radiology reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohammad/opt/anaconda3/envs/radiopaths/lib/python3.7/site-packages/ipykernel_launcher.py:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  if sys.path[0] == '':\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:33<00:00,  2.95it/s]\n"
     ]
    }
   ],
   "source": [
    "def load_and_scale(rel_path):\n",
    "    remote_path = os.path.join(base_url, \"mimic-cxr-jpg/2.0.0/\", rel_path)\n",
    "    local_path = os.path.join(local_dir, rel_path)\n",
    "    !wget -q --user {user} --password {password} {remote_path} -P {os.path.dirname(local_path)}\n",
    "    image = Image.open(local_path)\n",
    "    image = image.resize((image_resolution, image_resolution), Image.LANCZOS)\n",
    "    image.save(local_path)\n",
    "\n",
    "with Pool(cpu_count()) as p:\n",
    "    image_list = mimic_cxr.image_path.str.replace(\".dcm\", \".jpg\").to_list()[:100]\n",
    "    list(tqdm(p.imap(load_and_scale, image_list), total=len(image_list)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 ('radiopaths')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7 (default, May  7 2020, 21:25:33) \n[GCC 7.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "03acefbb53fcc115c049ab1dd80d89e90c42faf251ff4b9cb79a9ebd3c9b7ade"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
